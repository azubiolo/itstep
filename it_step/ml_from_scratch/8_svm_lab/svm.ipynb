{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Support Vector Machines\n",
    "## Course recap\n",
    "This lab consists in implementing the **Support Vector Machines** (SVM) algorithm. \n",
    "\n",
    "Given a training set $ D = \\left\\{ \\left(x^{(i)}, y^{(i)}\\right), x^{(i)} \\in \\mathcal{X}, y^{(i)} \\in \\mathcal{Y}, i \\in \\{1, \\dots, n \\}  \\right\\}$, where $\\mathcal{Y} = \\{ 1, \\dots, k\\}$ . Recall (from lecture 7), SVM aims at minimizing the following cost function $J$:\n",
    "$$\n",
    "\\begin{split}\n",
    "J(\\theta_1, \\theta_2, \\dots, \\theta_k) \n",
    "\t&= \\sum_{i = 1}^n L_i \\\\\n",
    "\t&= \\sum_{i = 1}^n \\sum_{j \\neq y_i} \\max(0, \\theta_j^Tx^{(i)} - \\theta_{y^{(i)}}^T x^{(i)} + \\Delta)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Defining the training set\n",
    "Let us define variables `X` and `Y` that will contain the features $\\mathcal{X}$ and labels $\\mathcal{Y}$ of the training set. Again, we will be having an intercept $x_0^{(i)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k_classes = 2\n",
    "X = [[1., 1.5, 0.2], [1., 0.3, 1.2], [1, 1.6, 0.4], [1., 1.3, 0.25], [1., 0.5, 1.12]]\n",
    "Y = [1, 2, 1, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us take a look at the data in 2D (we ignore the intercept which is constantly equal to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8cc0449780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "X1 = [x[1] for x in X]\n",
    "X2 = [x[2] for x in X]\n",
    "plt.scatter(X1, X2, c=Y) # plot x1, x2, color is defined by the label y\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data was generated so that we have two quite distinct classes. This is usually not the case in reality, and for this reason we will see what happens when outliers are implemented (see homework below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prediction function\n",
    "**Exercise**: Define a function `score` that takes as parameter *the feature vector* $x$ as well as *a model* $\\theta$ and outputs the score:\n",
    "$$ h(x) = \\theta^T x = \\sum_{j = 0}^d \\theta_j x_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def score(x, theta):\n",
    "    d = len(x)\n",
    "    thetaTx = 0\n",
    "    for idx in range(d):\n",
    "        thetaTx += x[idx] * theta[idx]\n",
    "    return thetaTx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Defining the cost function\n",
    "### Cost function on a single sample\n",
    "**Exercise**: Define a function `cost_function` that takes as parameter a sample (*the actual label* $y$, the feature vector $x$), the $\\theta$s for each classes as well as $\\Delta$ and returns the value of the cost function for this sample. \n",
    "\n",
    "**Hint**: Recall from lecture 7 that it is given by:\n",
    "$$ L_i = \\sum_{j \\neq y_i} \\max(0, \\theta_j^Tx - \\theta_{y}^T x + \\Delta) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cost_function(x, y, thetas, delta):\n",
    "    thetayTx = predict(x, thetas[y])\n",
    "    loss = 0\n",
    "    d = len(x)\n",
    "    for j in range(d):\n",
    "        if j is not y:\n",
    "            print(\"x \" + str(x))\n",
    "            print(\"thetas \" + str(thetas))\n",
    "            thetajTx = predict(x, thetas[idx])\n",
    "            loss += max(0, thetajTx - thetayTx + delta)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we are able to compute the loss for a single training sample, we can get the total cost.\n",
    "\n",
    "**Exercise**: Define a function `cost_function_total` that will compute the total cost function given by\n",
    "$$ J = L_i = \\sum_{i = 1}^n \\sum_{j \\neq y_i} \\max(0, \\theta_j^Tx^{(i)} - \\theta_{y^{(i)}}^T x^{(i)} + \\Delta) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cost_function_total(X, Y, thetas, delta):\n",
    "    cost = 0 # initialize the cost with 0\n",
    "    n = len(Y)\n",
    "    for i in range(n): # iterate over the training set\n",
    "        x = X[i] # get the ith feature vector\n",
    "        y = Y[i] # get the ith label\n",
    "        cost += cost_function(x, y, thetas, delta) # add the cost of the current sample to the total cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def initialize_thetas(X, k_classes):\n",
    "    d = len(X[1])\n",
    "    theta = [0] * d\n",
    "    return [theta] * k_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "thetas_0 = initialize_thetas(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Recall that the prediction on a feature vector $x$ is given by the value of $j$ that maximizes the score $\\theta_j^T x$.\n",
    "\n",
    "**Exercise**: Define a function `predict` which takes a feature vector `x` as well as the $\\theta_j$s and outputs the predicted class $\\hat{y}$ which is the value of $j$ maximizing the score.\n",
    "\n",
    "**Hint**: We have defined a `score` function for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict(x, thetas):\n",
    "    k_classes = len(thetas)\n",
    "    prediction = 0\n",
    "    highest_score = score(x, thetas[prediction]) # initialize with the first class\n",
    "    for idx_class in range(k_classes):\n",
    "        class_score = score(x, thetas[idx_class])\n",
    "        if class_score > highest_score:\n",
    "            prediction = idx_class\n",
    "    return prediction + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X[0], thetas_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gradient\n",
    "\n",
    "We have just defined everything we need to make the prediction and compute the loss for the SVM problem. As usual, we want to minimize this loss. The **gradient descent** works well in this case and in order to apply it, we need to compute the gradient.\n",
    "\n",
    "Recall that we need to compute a gradient per class as we have $k$ vectors $\\theta_j$. The gradient for a sample $x, y$ is given by the following formulas:\n",
    "- if $j \\neq y$:\n",
    "$$\n",
    "\\nabla_{\\theta_j} L = \n",
    "\t\\begin{cases}\n",
    "\tx \\quad \\text{if} \\quad \\theta_j^Tx - \\theta_{y}^Tx + \\Delta > 0 \\\\\n",
    "\t0 \\quad \\text{otherwise.}\n",
    "\t\\end{cases}\n",
    "$$\n",
    "- if $j = y$:\n",
    "$$\n",
    "\\nabla_{\\theta_y} L = px\n",
    "$$\n",
    "where $p$ is the **number of times the desired margin is not satisfied**, that is, the number of $j \\neq y$ such that $\\theta_j^Tx - \\theta_{y}^Tx + \\Delta > 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gradients(x, y, thetas, delta):\n",
    "    d = len(x)\n",
    "    k_classes = len(thetas)\n",
    "    predicted_class = predict(x, thetas)\n",
    "    grads = [[0] * d] * k_classes # initialize a list of k_class gradients with zeros everywhere\n",
    "    for idx_class in range(k_classes): # iterate over all the classes to compute the gradient for each class\n",
    "        # there are 2 formulas: one for the true class (given by 'y') and another one for the other classes\n",
    "        if idx_class + 1 == y: # if idx_class is equal to the actual class\n",
    "            p = 0\n",
    "            for j in range(k_classes):\n",
    "                if j + 1 != y: # are counting over the classes different than the actual class\n",
    "                    if score(x, thetas[j]) - score(x, thetas[y - 1]) + delta > 0:\n",
    "                        p += 1\n",
    "            for idx in range(d):\n",
    "                grads[idx_class][idx] = - p * x[idx]\n",
    "        else: # if idx_class is not the actual class\n",
    "            if score(x, thetas[idx_class]) - score(x, thetas[y - 1]) + delta > 0:\n",
    "                for idx in range(d):\n",
    "                    grads[idx_class][idx] = x[idx]\n",
    "            # we do not need an else statement here because the gradient would be equal to 0 in this case, \n",
    "            # and the gradient has been initialized with zeros\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 1.5, 0.2], [1.0, 1.5, 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# to delete\n",
    "print(gradients(X[0], Y[0], thetas_0, 4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The last quantity needed in order to apply the gradient descent is the total gradient (if we want to apply a batch gradient descent, the gradient for a single sample is enough in the stochastic gradient descent case is enough).\n",
    "\n",
    "To compute it, we just need to sum the gradients for all the samples within the training set.\n",
    "\n",
    "**Exercise**: Implement a function `gradient_total` that takes as inputs a set of feature vectors $X$, a set of labels $Y$, values for the $\\theta_j$s as well as the hyperparamter $\\Delta$ and outputs the gradient of $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For the sake of clarity, we first define a function that sums vectors elementwise\n",
    "def sum_vectors(x1, x2):\n",
    "    d = len(x1)\n",
    "    sum_vector = x1\n",
    "    for idx in range(d):\n",
    "        sum_vector[idx] += x2[idx]\n",
    "    return sum_vector\n",
    "\n",
    "def gradient_total(X, Y, thetas, delta):\n",
    "    n = len(Y) # number of training samples\n",
    "    d = len(X[1])\n",
    "    k_classes = len(thetas)\n",
    "    grads_sum = [[0] * d] * k_classes \n",
    "    for i in range(n):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        grads = gradients(x, y, thetas, delta) # get the gradient for the current sample\n",
    "        for j in range(k_classes):\n",
    "            grads_sum[j] = sum_vectors(grads[j], grads_sum[j]) # add it to the total gradients\n",
    "    return grads_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13.0, 30.1, -5.420000000000001], [13.0, 30.1, -5.420000000000001]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to delete\n",
    "gradient_total(X, Y, thetas_0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have the gradient, we can apply the gradient descent algorithm.\n",
    "\n",
    "**Exercise**: Implement a function `gradient_descent` that takes as parameter the training set $(X, Y)$, the hyperparameter $\\Delta$ as well as a learning rate and applied the gradient descent algorithm to the SVM case.\n",
    "\n",
    "**Hint**: Feel free to get back to the lectures to recall the gradient descent update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For the sake of readability, we define a function axpb (that stands for a x plus b) that outputs a * x + b \n",
    "# where a is a scalar and x and b are vectors\n",
    "def axpb(a, x, b):\n",
    "    # x and b should have the same size, a is a scalar\n",
    "    d = len(x)\n",
    "    sum_vector = b\n",
    "    for idx in range(d):\n",
    "        sum_vector[idx] += a * x[idx]\n",
    "    return sum_vector\n",
    "\n",
    "def gradient_descent(X, Y, delta, learning_rate):\n",
    "    k_classes = len(set(Y))\n",
    "    thetas = initialize_thetas(X, k_classes)\n",
    "    for i_iter in range(5):\n",
    "        grads = gradient_total(X, Y, thetas, delta)\n",
    "        for j in range(k_classes):\n",
    "            thetas[j] = axpb(-learning_rate, grads[j], thetas[j])\n",
    "        print(\"X \" + str(X))\n",
    "        cost = cost_function_total(X, Y, thetas, delta)\n",
    "        print(\"iteration \" + str(i_iter) + \", cost = \" + str(cost))\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Homework\n",
    "At this point, you should be able to guess the questions for the homework and solve them.\n",
    "As for the OLS case, the questions are:\n",
    "- Define initial vectors $\\theta_j$s (for example with only zeros) and apply the gradient descent to the simple data set we have defined above. Do not forget to play with the learning rate and well as $\\Delta$\n",
    "- Do the same with the stochastic gradient descent version.\n",
    "- Observe how the space is partitionned after having trained the SVM model.\n",
    "- For both solutions above, observe the loss function in 2D, and observe the path followed by the $\\theta$ values over time.\n",
    "- What does an outlier look like in the classification case? Add an outlier to the training set, and run the SVM algorithm to it. What issue do we observe? Propose a solution to overcome this issue and implement it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
