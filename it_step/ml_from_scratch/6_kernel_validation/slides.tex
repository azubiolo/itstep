\documentclass{beamer}
%\documentclass[aspectratio=169]{beamer}
%
\mode<presentation>
{
  \usetheme{default}      
  \usecolortheme{default}
  \usefonttheme{default} 
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{bbm}
\usepackage{bm}

\newcommand{\1}[1]{\mathbbm{1}\left[#1\right]}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\yi}{y^{(i)}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\yhati}{\hat{y}^{(i)}}
\newcommand{\bx}{\bm{x}}

\title{Machine learning from scratch}
\subtitle{Lecture 6: Non-linear models, parameter selection}
\author{Alexis Zubiolo\newline\texttt{alexis.zubiolo@gmail.com}}
\institute{Data Science Team Lead @ Adcash}
\date{March 9, 2017}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Course outline}
Last time, we reviewed the main ideas behind OLS
\vfill
\pause
This lecture will go a bit further by introducing:
\begin{itemize}
	\item Regularization
	\item Non linear models (polynomial kernels)
	\item Model evaluation
	\item Parameter selection
\end{itemize}
\end{frame}

\begin{frame}
	\center
	\huge{Regularizing models}
\end{frame}

\begin{frame}{Outliers and overfitting}
Recall from previous course: Outliers can ruin the trained model.
\pause
\vfill
\begin{figure}
\centering
\includegraphics[width=\linewidth]{images/line_regression_outlier.png}
\end{figure}
\end{frame}

\begin{frame}{Outliers and overfitting}
When this happens, we usually notice that one of the weight is big (in this case, $\theta_1$).
\pause
\vfill
How to avoid that? \pause By \textbf{penalizing big weights} in the model.
\pause
\vfill 
Formally, this consists in rewriting the cost function $J(\theta)$. Initially, we had 
\begin{equation*}
J(\theta) = \dfrac{1}{2} \sum_{i = 1}^{n} \ell \left( \yhati, \yi \right)
\end{equation*}
\pause
\vfill 
We can add another term $R(\theta)$ called \textbf{regularization}
\begin{equation*}
J(\theta) = \dfrac{1}{2} \sum_{i = 1}^{n} \ell \left( \yhati, \yi \right) + \lambda R(\theta)
\end{equation*}
\pause
\vfill
where $\lambda$ is a \textbf{hyper-parameter} that quantifies how much we want to penalize big values of $\theta$.
\end{frame}


\begin{frame}{Outliers and overfitting}
In the end, $J(\theta)$ is as follows:
\begin{equation*}
J(\theta) = L(\theta) + \lambda R(\theta)
\end{equation*}
where 
\begin{itemize}
	\item $L$ is the \textbf{loss term}
	\item $R$ is the \textbf{regularization term}
	\item $\lambda$ is the \textbf{regularization parameter}
\end{itemize}
\pause
\vfill
A commonly used regularization term is often the squared $\ell_2$-norm, given by:
\begin{equation*}
R(\theta) = \norm{\theta}_2^2 = \sum_{j = 1}^d \theta_j^2
\end{equation*}
\end{frame}

\begin{frame}{$\ell_2$-regularized OLS}
OLS with this regularization term becomes:
\begin{equation*}
J(\theta) = L(\theta) + \lambda R(\theta)
\end{equation*}
\vfill
\pause
This is the new function we have to \textbf{minimize}. Once again, we can use a \textbf{gradient descent} algorithm. This means we need to \textbf{compute the gradient}. By linearity of the gradient operator, we have:
\begin{equation*}
\nabla J(\theta) = \nabla L(\theta) + \lambda \nabla R(\theta)
\end{equation*}
\pause
We already know $\nabla L(\theta)$, so we just need to find $\nabla R(\theta)$ in order to have $\nabla J(\theta)$. 
\vfill
\pause
\textbf{Exercice}: Recall
\begin{equation*}
R(\theta) = \norm{\theta}_2^2 = \sum_{j = 1}^d \theta_j^2
\end{equation*}
What is $\nabla R(\theta)$?
\end{frame}

\begin{frame}{$\ell_2$-regularized OLS}
\textbf{Hint}: Recall the definition of the gradient:
\begin{equation*}
\nabla R(\theta) = \left[ \dfrac{\partial}{\partial \theta_1} R(\theta), \dots, \dfrac{\partial}{\partial \theta_d} R(\theta) \right]
\end{equation*}
\vfill
\pause
\textbf{Solution}: 

\begin{equation*}
\begin{split}
\dfrac{\partial}{\partial \theta_k} R(\theta) &= \dfrac{\partial}{\partial \theta_k} \sum_{j = 1}^d \theta_j^2 \\
&= \sum_{j = 1}^d \dfrac{\partial}{\partial \theta_k} \theta_j^2 \\
&= 2 \theta_j
\end{split}
\end{equation*}
\textbf{Conclusion}:
\begin{equation*}
\nabla R(\theta) = \left[ 2\theta_1, \dots, 2\theta_d \right]
\end{equation*}
\end{frame}

\begin{frame}{$\ell_2$-regularized OLS}
\textbf{Question}: Given
\begin{equation*}
\nabla R(\theta) = \left[ 2\theta_1, \dots, 2\theta_d \right]
\end{equation*}
What is gradient descent update rule?
\pause
\vfill
\textbf{Hint}: Recall the gradient descent update rule:
\begin{equation*}
\theta_k := \theta_k - \alpha \dfrac{\partial}{\partial \theta_k} J(\theta)
\end{equation*}

\textbf{Solution}: We have 
\begin{equation*}
\dfrac{\partial}{\partial \theta_k} L(\theta) = \left( y - h\left(\bx\right)\right) x_k \qquad \text{and} \qquad
\dfrac{\partial}{\partial \theta_k} R(\theta) = 2\theta_k
\end{equation*}
so
\begin{equation*}
\theta_k := \theta_k - \alpha (\left( y - h\left(\bx\right)\right) x_k - 2 \lambda \theta_k)
\end{equation*}
\end{frame}

\begin{frame}
	\center
	\huge{More sophisticated models}
\end{frame}

\begin{frame}{Non-linearity}
We might want to apply OLS to this data:
\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{images/underfitting.png}
\end{figure}
\end{frame}

\begin{frame}{Non-linearity}
We'd rather try to fit \textbf{a polynomial} on the data:
\begin{equation*}
\yhat = \theta_0 + \theta_1 x + \theta_2 x^2 + \dots + \theta_d x^d = \sum_{k = 0}^d \theta_d x^d 
\end{equation*}
\pause
\vfill
Se need to find $\theta_0, \theta_1, \dots, \theta_d$, which can be achieved by defining a new feature vector $\phi(x) = [1, x, x^2, \dots, x^d]$ and define the prediction model
\begin{equation*}
\yhat = \theta^T h(\phi(x)) = \sum_{k = 0}^d \theta_d x^d 
\end{equation*}
\pause
\vfill
OLS can train $\theta$ and we have a \textbf{polynomial model}. The trick is to apply a \textbf{non-linearity mapping} $\phi$.
\end{frame}

\begin{frame}{Non-linearity}
\textbf{Problem}: We would need to find an appropriate value for the degree $d$, because:
\vfill
\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/over_under_fitting.png}
\end{figure}
\end{frame}

\begin{frame}{Hyperparameters}
This is a \textbf{more general problem}: \textbf{Tuning hyperparameters}. The current version of OLS we have has several parameters:
\begin{itemize}
	\item The regularization parameter $\lambda$
	\item The kernel degree $d$
	\item The learning rate $\alpha$
\end{itemize}
\vfill
\pause
As we've seen, all these 3 parameters can have a \textbf{dramatic impact} on the quality of our prediction model! Hence, we need to \textbf{tune them} properly.
\end{frame}

\begin{frame}
	\center
	\huge{Model evaluation\\ Parameter selection}
\end{frame}

\begin{frame}{Train-test split}
As we saw with OLS, ML algorithms usually rely on \textbf{many parameters}. How to \textbf{tune} them properly given a data set?
\vfill
\pause
The most commonly used principle is the \textbf{train-test split}:
\begin{itemize}
	\item \textbf{Split the data} into a training set and a test set
	\item \textbf{Train} on the training set
	\item \textbf{Test} on the test set
\end{itemize}
\vfill
\pause
This is often referred to as \textbf{cross-validation}.
\end{frame}

\begin{frame}{Cross-validation}
Standard technique: Hold-out cross-validation:
\begin{itemize}
	\item Train on a part of the data (e.g. 70\%)
	\item Test on the remaining data (e.g. 30\%)
\end{itemize}
\pause
\vfill
Another standard technique: \textbf{$k$-fold cross-validation}
\begin{itemize}
	\item Split the data into $k$ (equally-sized) folds
	\item Remove 1 fold (= test fold)
	\item Train on the other folds
	\item Test on the removed fold
	\item Do it for all the folds
\end{itemize}
\end{frame}

\begin{frame}{Small data set}
Suppose you have \textbf{a small data set}. How to evaluate a classifier in this case?
\vfill
\pause
$k$-fold cross-validation? Even with $k = 2$, it would make the training set even smaller and make it hard to fit a proper model.
\vfill
\pause
Other option: \textbf{Leave-one-out} (LOO) cross-validation:
\pause
\begin{itemize}
	\item \textbf{Remove 1 sample} from the data set
	\item Train on \textbf{all the other samples}
	\item Test on the sample you've removed
	\item \textbf{Evaluate} the prediction
	\item Do it \textbf{for each sample of the data set}
	\item \textbf{Aggregate} the evaluations
\end{itemize}
\pause
\vfill
\textbf{Remark}: This could lead to many iterations even if the data set is small.
\pause
\vfill
\textbf{Alternative}: Leave-$p$-out (LPO). LOO is LPO with $p = 1$.
\end{frame}

\begin{frame}{Parameter selection}
One of the goals of model evaluation is to select a \textbf{good model}.
\vfill
\pause
Hence we want to choose one (or several) parameters. How do we do?
\vfill
\pause
Most classic way: a \textbf{grid-search}
\begin{itemize}
	\item For each parameter, define a set of possible values
	\item For each parameter combination, train/test the model
	\item Pick the parameter combination which gives best results
\end{itemize}
\vfill
\pause
\textbf{Important note}: Hyper-parameter ranges vary a lot from an application to another. It is \textbf{data-dependent}.
\end{frame}

\begin{frame}{Grid search vs random search}
Random search is a more and more popular alternative to grid search, especially in this case:
\vfill
\pause
\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/random_search.png}
\end{figure}
\pause
\vfill
In any case, you need to guess upper/lower bounds on the parameters.
\pause
\vfill
\textbf{Practical note}: Each parameter combination can be trained/tested separately =$>$ possibility to distribute the tasks
\end{frame}

\begin{frame}{Conclusion}
In this lecture, we've seen:
\begin{itemize}
	\item How to avoid overfitting by regularizing the model
	\item How to evaluate models
	\item How to tune parameters automatically
\end{itemize}
\vfill
\pause
During the next lecture, we will work on implementing regularization to the OLS algorithm and cross-validating it and switch to classification if the time allows it.
\end{frame}

\begin{frame}
	\center
	\huge{Thank you! Questions?}
\end{frame}

\end{document}